# Awesome Video Reasoning

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

> A curated list of papers and resources focused on video reasoning capabilities of AI models.

## Papers

### 2025

**RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence** | *Dec 2025*  
Xuming He, Zehao Fan, Hengjia Li, Fan Zhuo, Hankun Xu, Senlin Cheng, Di Weng, Haifeng Liu, Can Ye, Boxi Wu  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.02622) | ðŸ’» [Project](https://hexmseeu.github.io/RULER-Bench-proj/)

**In-Video Instructions: Visual Signals as Generative Control** | *Nov 2025*  
Gongfan Fang, Xinyin Ma, Xinchao Wang  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.19401) | ðŸ’» [Project](https://fangggf.github.io/In-Video/)

**Video Generation Models Are Good Latent Reward Models** | *Nov 2025*  
Xiaoyue Mi, Wenqing Yu, Jiesong Lian, Shibo Jie, Ruizhe Zhong, Zijun Liu, Guozhen Zhang, Zixiang Zhou, Zhiyong Xu, Yuan Zhou, Qinglin Lu, Fan Tang  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.21541)

**Video4Edit: Viewing Image Editing as a Degenerate Temporal Process** | *Nov 2025*  
Xiaofan Li, Yanpeng Sun, Chenming Wu, Fan Duan, YuAn Wang, Weihao Bo, Yumeng Zhang, Dingkang Liang  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.18131)

**VChain: Chain-of-Visual-Thought for Reasoning in Video Generation** | *Oct 2025*  
Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu, Paul Debevec, Ziwei Liu  
ðŸ“„ [Paper](https://arxiv.org/abs/2510.05094) | ðŸ’» [Project](https://eyeline-labs.github.io/VChain/)

**Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark** | *Nov 2025*  
Xinxin Liu, Zhaopan Xu, Kai Wang, Yong Jae Lee, Yuzhang Shang  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.13853) | ðŸ’» [Code](https://github.com/L-CodingSpace/GVR)

**Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark** | *Oct 2025*  
Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng  
ðŸ“„ [Paper](https://arxiv.org/abs/2510.26802) | ðŸ’» [Code](https://github.com/ZiyuGuo99/MME-CoF)

**Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven's Matrices**  
Hokin Deng  
ðŸ“„ [Paper](https://github.com/hokindeng/VMEvalKit/blob/main/paper/video-models-start-to-solve/Video_Model_Start_to_Solve.pdf) | ðŸ’» [Code](https://github.com/hokindeng/VMEvalKit)

**Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO** | *Nov 2025*  
Junhao Cheng, Liang Hou, Xin Tao, Jing Liao  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.16669) | ðŸ’» [Code](https://github.com/KlingTeam/VANS)

**Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks** | *Nov 2025*  
Cheng Yang, Haiyuan Wan, Yiran Peng, Xin Cheng, Zhaoyang Yu, Jiayi Zhang, Junchi Yu, Xinlei Yu, Xiawu Zheng, Dongzhan Zhou, Chenglin Wu  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.15065) | ðŸ’» [Code](https://github.com/ImYangC7/VR-Bench)

**Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm** | *Nov 2025*  
Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.04570) | ðŸ’» [Code](https://github.com/tongjingqi/Thinking-with-Video)

**V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models** | *Nov 2025*  
Yang Luo, Xuanlei Zhao, Baijiong Lin, Lingting Zhu, Liyao Tang, Yuqi Liu, Ying-Cong Chen, Shengju Qian, Xin Wang, Yang You  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.16668) | ðŸ’» [Code](https://github.com/yangluo7/V-ReasonBench)

**TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models** | *Nov 2025*  
Harold Haodong Chen, Disen Lan, Wen-Jie Shu, Qingyang Liu, Zihan Wang, Sirui Chen, Wenkai Cheng, Kanghao Chen, Hongfei Zhang, Zixin Zhang, Rongjin Guo, Yu Cheng, Ying-Cong Chen  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.13704) | ðŸ’» [Code](https://github.com/EnVision-Research/TiViBench)

**Video models are zero-shot learners and reasoners** | *Sep 2025*  
ThaddÃ¤us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos  
ðŸ“„ [Paper](https://arxiv.org/abs/2509.20328) | ðŸ’» [Project](https://video-zero-shot.github.io/)

> **Note:** This repository **only includes works on reasoning with video models**. In general, we do **not** list works on language models or general multi-modal language models. 

## Contributing

Contributions are welcome! Please feel free to submit a pull request to add new papers or resources.

## License

This project is licensed under the Apache-2.0 License - see the [LICENSE](LICENSE) file for details.
