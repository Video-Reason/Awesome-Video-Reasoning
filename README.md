# Awesome Video Reasoning

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

> This repository **only includes works on reasoning with video models**. In general, we do **not** list works on language models or general multi-modal language models. 

## Papers

### 2025

**Spatia: Video Generation with Updatable Spatial Memory** | *Dec 2025*  
Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu, Yan Lu  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.15716) | ðŸ’» [Project](https://zhaojingjing713.github.io/Spatia/)

**MMGR: Multi-Modal Generative Reasoning** | *Dec 2025*  
Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou, Kung-Hsiang Huang, Parisa Kordjamshidi, Minjia Zhang, Wen Xiao, Jiuxiang Gu, Nanyun Peng, Junjie Hu  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.14691)

**Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation** | *Dec 2025*  
Zeqi Xiao, Yiwei Zhao, Lingxiao Li, Yushi Lan, Ning Yu, Rahul Garg, Roshni Cooper, Mohammad H. Taghavi, Xingang Pan  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.03040) | ðŸ’» [Project](https://xizaoqu.github.io/video4spatial/)

**WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling** | *Dec 2025*  
Yuta Oshima, Yusuke Iwasawa, Masahiro Suzuki, Yutaka Matsuo, Hiroki Furuta  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.02473)

**Evaluating Gemini Robotics Policies in a Veo World Simulator** | *Dec 2025*  
Gemini Robotics Team, Coline Devin, Yilun Du, Debidatta Dwibedi, Ruiqi Gao, Abhishek Jindal, Thomas Kipf, Sean Kirmani, Fangchen Liu, Anirudha Majumdar, Andrew Marmon, Carolina Parada, Yulia Rubanova, Dhruv Shah, Vikas Sindhwani, Jie Tan, Fei Xia, Ted Xiao, Sherry Yang, Wenhao Yu, Allan Zhou  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.10675)

**RELIC: Interactive Video World Model with Long-Horizon Memory** | *Dec 2025*  
Yicong Hong, Yiqun Mei, Chongjian Ge, Yiran Xu, Yang Zhou, Sai Bi, Yannick Hold-Geoffroy, Mike Roberts, Matthew Fisher, Eli Shechtman, Kalyan Sunkavalli, Feng Liu, Zhengqi Li, Hao Tan  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.04040)

**Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation** | *Dec 2025*  
Yunhong Lu, Yanhong Zeng, Haobo Li, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jiapeng Zhu, Hengyuan Cao, Zhipeng Zhang, Xing Zhu, Yujun Shen, Min Zhang  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.04678) | ðŸ’» [Project](https://reward-forcing.github.io/)

**Astra: General Interactive World Model with Autoregressive Denoising** | *Dec 2025*  
Yixuan Zhu, Jiaqi Feng, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.08931) | ðŸ’» [Code](https://github.com/EternalEvan/Astra)

**Saber: Scaling Zero-Shot Reference-to-Video Generation** | *Dec 2025*  
Zijian Zhou, Shikun Liu, Haozhe Liu, Haonan Qiu, Zhaochong An, Weiming Ren, Zhiheng Liu, Xiaoke Huang, Kam Woh Ng, Tian Xie, Xiao Han, Yuren Cong, Hang Li, Chuyan Zhu, Aditya Patel, Tao Xiang, Sen He  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.06905) | ðŸ’» [Project](https://franciszzj.github.io/Saber/)

**DDRL: Data-regularized Reinforcement Learning for Diffusion Models at Scale** | *Dec 2025*  
Haotian Ye, Kaiwen Zheng, Jiashu Xu, Puheng Li, Huayu Chen, Jiaqi Han, Sheng Liu, Qinsheng Zhang, Hanzi Mao, Zekun Hao, Prithvijit Chattopadhyay, Dinghao Yang, Liang Feng, Maosheng Liao, Junjie Bai, Ming-Yu Liu, James Zou, Stefano Ermon  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.04332)

**World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty** | *Dec 2025*  
Zhiting Mei, Tenny Yin, Micah Baker, Ola Shorinwa, Anirudha Majumdar  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.05927)

**Stable Video Infinity: Infinite-Length Video Generation with Error Recycling** | *Oct 2025*  
Wuyang Li, Wentao Pan, Po-Chien Luan, Yang Gao, Alexandre Alahi  
ðŸ“„ [Paper](https://arxiv.org/abs/2510.09212) | ðŸ’» [Project](https://stable-video-infinity.github.io/homepage/)

**Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training** | *May 2025*  
Tony Bonnaire, RaphaÃ«l Urfin, Giulio Biroli, Marc MÃ©zard  
ðŸ“„ [Paper](https://arxiv.org/abs/2505.17638)

**RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence** | *Dec 2025*  
Xuming He, Zehao Fan, Hengjia Li, Fan Zhuo, Hankun Xu, Senlin Cheng, Di Weng, Haifeng Liu, Can Ye, Boxi Wu  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.02622) | ðŸ’» [Project](https://hexmseeu.github.io/RULER-Bench-proj/)

**What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards** | *Nov 2025*  
Minh-Quan Le, Yuanzhi Zhu, Vicky Kalogeiton, Dimitris Samaras  
ðŸ“„ [Paper](https://arxiv.org/abs/2512.00425) | ðŸ’» [Project](https://cvlab-stonybrook.github.io/NewtonRewards/)

**In-Video Instructions: Visual Signals as Generative Control** | *Nov 2025*  
Gongfan Fang, Xinyin Ma, Xinchao Wang  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.19401) | ðŸ’» [Project](https://fangggf.github.io/In-Video/)

**Video Generation Models Are Good Latent Reward Models** | *Nov 2025*  
Xiaoyue Mi, Wenqing Yu, Jiesong Lian, Shibo Jie, Ruizhe Zhong, Zijun Liu, Guozhen Zhang, Zixiang Zhou, Zhiyong Xu, Yuan Zhou, Qinglin Lu, Fan Tang  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.21541)

**Video4Edit: Viewing Image Editing as a Degenerate Temporal Process** | *Nov 2025*  
Xiaofan Li, Yanpeng Sun, Chenming Wu, Fan Duan, YuAn Wang, Weihao Bo, Yumeng Zhang, Dingkang Liang  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.18131)

**VChain: Chain-of-Visual-Thought for Reasoning in Video Generation** | *Oct 2025*  
Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu, Paul Debevec, Ziwei Liu  
ðŸ“„ [Paper](https://arxiv.org/abs/2510.05094) | ðŸ’» [Project](https://eyeline-labs.github.io/VChain/)

**Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark** | *Nov 2025*  
Xinxin Liu, Zhaopan Xu, Kai Wang, Yong Jae Lee, Yuzhang Shang  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.13853) | ðŸ’» [Code](https://github.com/L-CodingSpace/GVR)

**Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark** | *Oct 2025*  
Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng  
ðŸ“„ [Paper](https://arxiv.org/abs/2510.26802) | ðŸ’» [Code](https://github.com/ZiyuGuo99/MME-CoF)

**Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven's Matrices**  
Hokin Deng  
ðŸ“„ [Paper](https://github.com/hokindeng/VMEvalKit/blob/main/paper/video-models-start-to-solve/Video_Model_Start_to_Solve.pdf) | ðŸ’» [Code](https://github.com/hokindeng/VMEvalKit)

**Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO** | *Nov 2025*  
Junhao Cheng, Liang Hou, Xin Tao, Jing Liao  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.16669) | ðŸ’» [Code](https://github.com/KlingTeam/VANS)

**Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks** | *Nov 2025*  
Cheng Yang, Haiyuan Wan, Yiran Peng, Xin Cheng, Zhaoyang Yu, Jiayi Zhang, Junchi Yu, Xinlei Yu, Xiawu Zheng, Dongzhan Zhou, Chenglin Wu  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.15065) | ðŸ’» [Code](https://github.com/ImYangC7/VR-Bench)

**Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm** | *Nov 2025*  
Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.04570) | ðŸ’» [Code](https://github.com/tongjingqi/Thinking-with-Video)

**V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models** | *Nov 2025*  
Yang Luo, Xuanlei Zhao, Baijiong Lin, Lingting Zhu, Liyao Tang, Yuqi Liu, Ying-Cong Chen, Shengju Qian, Xin Wang, Yang You  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.16668) | ðŸ’» [Code](https://github.com/yangluo7/V-ReasonBench)

**TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models** | *Nov 2025*  
Harold Haodong Chen, Disen Lan, Wen-Jie Shu, Qingyang Liu, Zihan Wang, Sirui Chen, Wenkai Cheng, Kanghao Chen, Hongfei Zhang, Zixin Zhang, Rongjin Guo, Yu Cheng, Ying-Cong Chen  
ðŸ“„ [Paper](https://arxiv.org/abs/2511.13704) | ðŸ’» [Code](https://github.com/EnVision-Research/TiViBench)

**Video models are zero-shot learners and reasoners** | *Sep 2025*  
ThaddÃ¤us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos  
ðŸ“„ [Paper](https://arxiv.org/abs/2509.20328) | ðŸ’» [Project](https://video-zero-shot.github.io/)

## Contributing

Contributions are welcome! Please feel free to submit a pull request to add new papers or resources.

## License

This project is licensed under the Apache-2.0 License - see the [LICENSE](LICENSE) file for details.
